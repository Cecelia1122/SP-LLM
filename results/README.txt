# Results

This folder contains artifacts produced by different phases of the project. What appears here depends on how you ran the pipeline.

## Quick reference

- Quick run (`python run_complete_project.py --quick`):
  - Trains Classical models only.
  - Skips Speech LLM training and skips Comparison.
  - You’ll see Classical-only artifacts.

- Full run (`python run_complete_project.py`):
  - Trains both Classical and Speech LLM models.
  - Runs the Comparison and generates comprehensive figures and reports.

## Artifacts by phase

### Classical approach (appears in both Quick and Full runs)
- classical_models_comparison.png — Bar charts and ROC curves for SVM / RF / MLP on validation.
- confusion_matrix.png — Confusion matrix for the best Classical model on the test set.
- feature_importance.png — Top feature importances (Random Forest).
- classical_performance_tracking.json — Metrics log for Classical experiments.

Generated by:
- Training: `python classical_approach.py`
- Or via orchestration: `python run_complete_project.py` (Phases 1–2)

### Speech LLM approach (Full runs only)
- speech_llm_performance.png — Accuracy/Precision/Recall/F1/AUC, confusion matrix, ROC, and training time.
- speech_llm_performance_tracking.json — Metrics log for the Speech LLM run.

Generated by:
- Training: `python speech_llm_approach.py`
- Or via orchestration: `python run_complete_project.py` (Phase 3)

Note: These files will not exist after `--quick`.

### Comparative analysis (Full runs only; requires both models)
- comprehensive_comparison.png — 9‑panel figure comparing Classical vs Speech LLM (metrics, ROC, speed, size, training time, summary table).
- detailed_comparison_report.txt — Human‑readable summary (winner, dataset info, deltas).
- model_comparison_summary.csv — Tabular summary of key metrics used in the figure.

Generated by:
- `python comparison_analysis.py`
- Or via orchestration: `python run_complete_project.py` (Phase 4)

Note: Skipped automatically if the Speech LLM model is not available.

## Reproduce

- Quick (Classical-only):
  - `python run_complete_project.py --quick`
- Full (Classical + Speech LLM + Comparison):
  - `python run_complete_project.py`

If you ran Quick first and later want the full comparison:
1) Train the Speech LLM: `python speech_llm_approach.py`
2) Run comparison: `python comparison_analysis.py`
