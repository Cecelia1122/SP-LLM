# Results

A single place to find and interpret the artifacts produced by the pipeline.
This folder is populated mainly by the Comparative Analysis phase.

- Quick run (`--quick`): Skips Speech LLM and Comparison — this folder can remain empty (expected).
- Full run: Trains both models and generates the comparison figure, report, and CSV summary.

## TL;DR — What you’ll see here

- comprehensive_comparison.png — Side‑by‑side Classical vs Speech LLM performance visualization
- detailed_comparison_report.txt — Human‑readable summary with key findings
- model_comparison_summary.csv — Tabular metrics used to build the figure

These files are produced only when both models are available (Full run).

## Run modes and what they produce

- Quick run (Classical only)
  - Command: `python run_complete_project.py --quick`
  - Outcome:
    - Trains Classical model
    - Skips Speech LLM training
    - Skips Comparison phase automatically
    - Results folder: likely empty (this is normal)

- Full run (Classical + Speech LLM + Comparison)
  - Command: `python run_complete_project.py`
  - Outcome:
    - Trains both models
    - Runs Comparative Analysis
    - Results folder: populated with the artifacts listed above

Tip: If the Speech LLM model is missing, the Comparison phase is auto‑skipped.

## Visual preview

Below is the main figure you’ll get after a full run:

![Comprehensive comparison](./comprehensive_comparison.png)

If you do not see this image after a full run, check the Troubleshooting section below.

## How to reproduce artifacts

- End‑to‑end (recommended)
  - Full: `python run_complete_project.py`
  - Quick (no comparison): `python run_complete_project.py --quick`

- If you ran Quick first and now want the comparison:
  1) Train LLM: `python speech_llm_approach.py`
  2) Generate comparison: `python comparison_analysis.py`

The comparison requires:
- models/classical_keyword_spotter.pkl
- models/speech_llm_model/config.json (+ associated weights)

## What’s inside each artifact

- comprehensive_comparison.png
  - Combines standard metrics (accuracy, precision, recall, F1, AUC), ROC curves, confusion matrices, speed, model size, and training time across approaches.
  - Lets you quickly compare trade‑offs between the Classical and Speech LLM methods.

- detailed_comparison_report.txt
  - Plain‑text narrative summarizing dataset stats, per‑model performance, and a “winner” callout with margin.

- model_comparison_summary.csv
  - Machine‑readable summary of the metrics used by the figure and report.
  - Open in your preferred spreadsheet tool for quick filtering/sorting.

Note: Depending on configuration, you may also see additional per‑approach plots generated by the training scripts (e.g., confusion matrices or ROC curves). These are optional and not required for the final comparison.

## Troubleshooting

- I ran a Full run but don’t see any files here
  - Ensure both models exist:
    - Classical: models/classical_keyword_spotter.pkl
    - LLM: models/speech_llm_model/config.json
  - If the LLM model is missing, the comparison will be skipped.
  - Check console output for any warnings or exceptions during the Comparison phase.

- Comparison says “skipped”
  - This is expected if either model is missing or you explicitly passed `--skip-comparison`.

- Training is slow
  - LLM training benefits significantly from a GPU. Consider using a GPU environment or Colab.

## Typical folder map (after Full run)

```
results/
├─ comprehensive_comparison.png
├─ detailed_comparison_report.txt
└─ model_comparison_summary.csv
```

For questions about how these files are produced, see the project’s main README and the comparison_analysis.py script.

—
Happy analyzing!
